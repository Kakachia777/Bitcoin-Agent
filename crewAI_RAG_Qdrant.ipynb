{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB_gQIe0XQgt"
   },
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AFRaCqZWkolC"
   },
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain import hub\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "from crewai_tools import BaseTool\n",
    "from crewai import Agent, Task, Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environmental variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "QDRANT_API_KEY = os.getenv('QDRANT_API_KEY')\n",
    "QDRANT_URL = os.getenv('QDRANT_URL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XH-vbBUQoG93"
   },
   "outputs": [],
   "source": [
    "# Load documents\n",
    "\n",
    "loader = PyPDFLoader(\"attention.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZWUH5E4Oq7qk"
   },
   "outputs": [],
   "source": [
    "# Split documents\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "document = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dcdJOnejrJ7A",
    "outputId": "1dfd765f-f9a9-4f88-d0ef-3563147e994d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗', metadata={'source': 'attention.pdf', 'page': 0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9eG-OjJ7rb6B",
    "outputId": "3aa2038e-7a2b-4c50-9951-cb24623b4489"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='tion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.', metadata={'source': 'attention.pdf', 'page': 1})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "63FoOb3mrqe_"
   },
   "outputs": [],
   "source": [
    "# Create collection in vector store\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY)\n",
    "\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"attention\",\n",
    "    vectors_config={\n",
    "        \"content\": VectorParams(size=1536, distance=Distance.COSINE)\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "8AHjREffuZxi"
   },
   "outputs": [],
   "source": [
    "# Upsert documents in vector store\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "def chunked_metadata(data, client=client, collection_name=\"attention\"):\n",
    "\n",
    "    chunked_metadata = []\n",
    "\n",
    "    for item in data:\n",
    "\n",
    "      id = str(uuid4())\n",
    "      content = item.page_content\n",
    "      source = item.metadata[\"source\"]\n",
    "      page = item.metadata[\"page\"]\n",
    "\n",
    "      content_vector = embedding.embed_documents([content])[0]\n",
    "      vector_dict = {\"content\": content_vector}\n",
    "\n",
    "      payload = {\n",
    "           \"page_content\": content,\n",
    "           \"metadata\": {\n",
    "                        \"id\": id,\n",
    "                        \"page_content\": content,\n",
    "                        \"source\": source,\n",
    "                        \"page\": page,\n",
    "                        }\n",
    "            }\n",
    "\n",
    "\n",
    "      metadata = PointStruct(id=id, vector=vector_dict, payload=payload)\n",
    "      chunked_metadata.append(metadata)\n",
    "\n",
    "    client.upsert(\n",
    "                  collection_name=collection_name,\n",
    "                  wait=True,\n",
    "                  points=chunked_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "zXYk8tvzuZ0U"
   },
   "outputs": [],
   "source": [
    "chunked_metadata(document[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FCDI12xeuZ3E",
    "outputId": "4dbe1cf5-57db-488b-8124-70e14b3916e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[CollectionDescription(name='attention')])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBI8-d3P92uA",
    "outputId": "a8cade9e-a359-4722-9e94-ca20ed51ee58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points: 90 \n"
     ]
    }
   ],
   "source": [
    "attention_collection = client.get_collection(\"attention\")\n",
    "\n",
    "print(f\"Points: {attention_collection.points_count} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zmETDxM_-iC0"
   },
   "outputs": [],
   "source": [
    "# client.delete_collection(collection_name=\"attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vN6CJRNI-8-m",
    "outputId": "7a7b23b2-c43e-4b1d-a22b-503142d5fa38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id='511d4b4b-72ea-42e4-954a-d5fa8decc60c', version=0, score=0.77504903, payload={'page_content': 'architectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer'}, vector=None, shard_key=None),\n",
       " ScoredPoint(id='0b73789c-151b-42e8-bc58-57aa00c299dd', version=1, score=0.77504903, payload={'page_content': 'architectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer'}, vector=None, shard_key=None),\n",
       " ScoredPoint(id='0c1978bb-726c-45c7-9e5d-a702bdbbf346', version=2, score=0.77504903, payload={'page_content': 'architectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer'}, vector=None, shard_key=None),\n",
       " ScoredPoint(id='0828ae47-0155-4202-b0a8-bcd2eb0d88cb', version=7, score=0.77504903, payload={'page_content': 'architectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer'}, vector=None, shard_key=None),\n",
       " ScoredPoint(id='c059f737-c539-4c50-8e00-9dac8456c1aa', version=4, score=0.77504903, payload={'page_content': 'architectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer'}, vector=None, shard_key=None)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search most similar documents to \"Positional Encoding\"\n",
    "\n",
    "client.search(\n",
    "    collection_name=\"attention\",\n",
    "    query_vector=(\"content\", embedding.embed_documents([\"Positional Encoding\"])[0]),\n",
    "    with_payload=[\"page_content\", \"source\"],\n",
    "    limit=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "dDZrcp8U-9Wd"
   },
   "outputs": [],
   "source": [
    "# Initialize vector store\n",
    "\n",
    "vectorstore = Qdrant(client=client,\n",
    "                     collection_name=\"attention\",\n",
    "                     embeddings=embedding,\n",
    "                     vector_name=\"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "29ICQc4TH7Hx"
   },
   "outputs": [],
   "source": [
    "# Downlad prompt template\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jn99i_7-IoVH",
    "outputId": "c62065be-bf52-4b87-cfee-4b04726abc62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))]\n"
     ]
    }
   ],
   "source": [
    "print(prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFolcjm-JHLW",
    "outputId": "b046609e-3e3e-497a-de3e-a88f7849dcf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: {question} \n",
      "Context: {context} \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "cuKc7sXhHN1g"
   },
   "outputs": [],
   "source": [
    "# Main parameters\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "\n",
    "Answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "handler =  StdOutCallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM gpt-3.5-turbo\n",
    "\n",
    "llm35 = ChatOpenAI(temperature=0.0,\n",
    "                 model=\"gpt-3.5-turbo\",\n",
    "                 max_tokens=512)\n",
    "\n",
    "\n",
    "# Retrieval qa chain\n",
    "qa_d35 = RetrievalQA.from_chain_type(\n",
    "                                    llm=llm35,\n",
    "                                    chain_type=\"stuff\",\n",
    "                                    callbacks=[handler],\n",
    "                                    retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The attention mechanism is a component used in sequence modeling and transduction tasks. It allows models to focus on different parts of the input sequence when making predictions, giving more weight to relevant information. This mechanism has become essential for improving the performance of models dealing with sequential data.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke qa_chain gpt-3.5-turbo\n",
    "\n",
    "qa_d35.invoke({\"query\": \"What is the attention mechanism?\"})[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "NLF2lOpVKN5t"
   },
   "outputs": [],
   "source": [
    "# LLM gpt-4o\n",
    "\n",
    "llm4o = ChatOpenAI(temperature=0.0,\n",
    "                 model=\"gpt-4o\",\n",
    "                 max_tokens=512)\n",
    "\n",
    "# Retrieval qa chain\n",
    "qa_db4o = RetrievalQA.from_chain_type(\n",
    "                                    llm=llm4o,\n",
    "                                    chain_type=\"stuff\",\n",
    "                                    callbacks=[handler],\n",
    "                                    retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "yIU1lxVyLd69",
    "outputId": "be09d984-d5f7-4984-8218-9644be8450dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The attention mechanism is a technique used in sequence modeling and transduction tasks, such as machine translation, text summarization, and image captioning. It allows the model to focus on different parts of the input sequence when generating each part of the output sequence. This mechanism helps the model to selectively concentrate on relevant parts of the input, thereby improving performance and efficiency.\n",
      "\n",
      "In essence, the attention mechanism assigns different weights to different parts of the input sequence, indicating their importance for the current step of the output sequence generation. This way, the model can dynamically adjust its focus based on the context, leading to better handling of long-range dependencies and more accurate predictions.\n",
      "\n",
      "Attention mechanisms have become a fundamental component in many state-of-the-art models, including the Transformer architecture, which has been highly successful in various natural language processing tasks.\n"
     ]
    }
   ],
   "source": [
    "print(qa_db4o.invoke({\"query\": \"What is the attention mechanism?\"})[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3GGnBkoUsC2",
    "outputId": "2b5ec1e5-3df1-425d-b744-a2303f4e6a84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Certainly! The research paper \"Attention Is All You Need\" by Ashish Vaswani et al. introduces the Transformer model, which has become a foundational architecture in the field of natural language processing (NLP). Here are some key points and analysis of the paper:\n",
      "\n",
      "### Key Contributions:\n",
      "1. **Transformer Architecture**:\n",
      "   - The paper introduces the Transformer model, which relies entirely on self-attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
      "   - This architecture allows for more parallelization during training, leading to significant speed improvements.\n",
      "\n",
      "2. **Self-Attention Mechanism**:\n",
      "   - The core innovation is the self-attention mechanism, which computes the representation of a word in a sequence by attending to all other words in the sequence.\n",
      "   - This mechanism allows the model to capture long-range dependencies more effectively than RNNs or CNNs.\n",
      "\n",
      "3. **Positional Encoding**:\n",
      "   - Since the Transformer model does not have a built-in notion of word order (unlike RNNs), the authors introduce positional encodings to inject information about the relative or absolute position of tokens in the sequence.\n",
      "\n",
      "4. **Multi-Head Attention**:\n",
      "   - The model uses multi-head attention to allow the model to jointly attend to information from different representation subspaces at different positions.\n",
      "\n",
      "5. **Layer Normalization and Residual Connections**:\n",
      "   - The architecture includes layer normalization and residual connections, which help in stabilizing the training and improving the gradient flow.\n",
      "\n",
      "### Impact:\n",
      "- The Transformer model has revolutionized NLP, leading to the development of models like BERT, GPT, and T5, which have set new benchmarks in various NLP tasks.\n",
      "- The architecture's ability to handle long-range dependencies and its efficiency in training have made it the go-to model for many NLP applications.\n",
      "\n",
      "### Strengths:\n",
      "- **Efficiency**: The parallelizable nature of the Transformer allows for faster training times compared to RNN-based models.\n",
      "- **Performance**: The model achieves state-of-the-art results on several NLP tasks.\n",
      "- **Flexibility**: The self-attention mechanism can be adapted to various tasks beyond NLP, such as image processing.\n",
      "\n",
      "### Weaknesses:\n",
      "- **Computationally Intensive**: Despite its efficiency in training, the Transformer can be computationally intensive, especially for very long sequences, due to the quadratic complexity of the self-attention mechanism.\n",
      "- **Data Hungry**: The model requires a large amount of data to train effectively, which can be a limitation in scenarios with limited data availability.\n",
      "\n",
      "### Conclusion:\n",
      "\"Attention\n"
     ]
    }
   ],
   "source": [
    "print(qa_db4o.invoke({\"query\": \"Can you analyze the research paper?\"})[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrewAI Agents and Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps are performed:\n",
    "\n",
    "- Search the research paper in the vector store as tool \n",
    "- Provide a detailed report with the researcher agent\n",
    "- Provide a short summary with the writer agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai_tools import BaseTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "ZJ5HW-6K7lob"
   },
   "outputs": [],
   "source": [
    "# Create custom tool to use the vector store\n",
    "\n",
    "class MyCustomTool(BaseTool):\n",
    "    name: str = \"Research Paper\"\n",
    "    description: str = \"Research Paper\"\n",
    "\n",
    "    def _run(self, argument: str) -> str:\n",
    "        return qa_db4o.run(\"Can you analyze the research paper?\")\n",
    "\n",
    "tool_instance = MyCustomTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y4zTQXEj9N8l",
    "outputId": "f583b1f2-ed7d-4c49-b598-fa0b0201b7ca",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 13:26:06,754 - 161784 - __init__.py-__init__:518 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Computer Science Researcher\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Conduct an in-depth analysis of the published research paper.\n",
      "                    Identify the key components and try to understand how each works.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mFirst, I need to read the research paper thoroughly to understand its content and key points.\n",
      "\n",
      "Action: Research Paper\n",
      "Action Input: {\"argument\": \"string\"}\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[95m \n",
      "\n",
      "I can provide an analysis of the research paper \"Attention Is All You Need\" by Ashish Vaswani et al. Here is a brief overview and analysis:\n",
      "\n",
      "### Overview\n",
      "\"Attention Is All You Need\" is a seminal paper in the field of natural language processing (NLP) and machine learning, introducing the Transformer model. The paper was authored by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, and Łukasz Kaiser, primarily affiliated with Google Brain and Google Research, with Aidan N. Gomez also affiliated with the University of Toronto.\n",
      "\n",
      "### Key Contributions\n",
      "1. **Transformer Model**: The paper introduces the Transformer model, which relies entirely on self-attention mechanisms, dispensing with the need for recurrent or convolutional layers traditionally used in sequence modeling tasks.\n",
      "\n",
      "2. **Self-Attention Mechanism**: The core innovation is the self-attention mechanism, which allows the model to weigh the importance of different words in a sentence when encoding a particular word. This mechanism enables the model to capture long-range dependencies more effectively than RNNs or CNNs.\n",
      "\n",
      "3. **Parallelization**: By removing recurrence, the Transformer model allows for more parallelization during training, significantly speeding up the process and making it more efficient on modern hardware.\n",
      "\n",
      "4. **Performance**: The Transformer model achieves state-of-the-art results on various NLP tasks, including machine translation, demonstrating its effectiveness and versatility.\n",
      "\n",
      "### Structure of the Paper\n",
      "1. **Introduction**: The paper begins with an introduction to the limitations of existing models and the motivation for developing the Transformer model.\n",
      "2. **Model Architecture**: Detailed description of the Transformer architecture, including the encoder and decoder components, multi-head self-attention, and position-wise feed-forward networks.\n",
      "3. **Self-Attention**: Explanation of the self-attention mechanism and how it is computed.\n",
      "4. **Positional Encoding**: Since the model does not inherently capture the order of words, positional encodings are added to the input embeddings to provide this information.\n",
      "5. **Training and Results**: Description of the training process, datasets used, and the results achieved by the Transformer model compared to other models.\n",
      "6. **Conclusion**: Summary of the findings and potential future directions for research.\n",
      "\n",
      "### Impact\n",
      "The introduction of the Transformer model has had a profound impact on the field of NLP and beyond. It has become the foundation for many subsequent models, including BERT, GPT, and\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mT5, leading to significant improvements in various NLP tasks. The self-attention mechanism has also been applied to other domains, such as computer vision.\n",
      "\n",
      "Thought: After thoroughly reviewing the research paper, I have extracted the main components and understood their workings. Now, I can compile a comprehensive analysis report as my final answer.\n",
      "Final Answer: \n",
      "Title: Attention Is All You Need\n",
      "Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser\n",
      "Affiliations: Google Brain, Google Research, University of Toronto\n",
      "\n",
      "Key Contributions:\n",
      "1. Introduced the Transformer model, which is based entirely on self-attention mechanisms and does not require recurrent or convolutional layers.\n",
      "2. Introduced the self-attention mechanism that allows the model to weigh the importance of different words when encoding a word, enabling the capturing of long-range dependencies more effectively.\n",
      "3. Enabled more parallelization during training by removing recurrence, speeding up the process and improving efficiency on modern hardware.\n",
      "4. Achieved state-of-the-art results on various NLP tasks, demonstrating the effectiveness and versatility of the Transformer model.\n",
      "\n",
      "Structure of the Paper:\n",
      "1. Introduction: Discussion on the limitations of existing models and the motivation for the Transformer model.\n",
      "2. Model Architecture: Detailed description of the Transformer architecture, including the encoder-decoder structure, multi-head self-attention, and position-wise feed-forward networks.\n",
      "3. Self-Attention: Explanation of the self-attention mechanism and its computation.\n",
      "4. Positional Encoding: Discussion on the addition of positional encodings to input embeddings to capture the order of words.\n",
      "5. Training and Results: Description of the training process, datasets used, and the comparative results achieved by the Transformer model.\n",
      "6. Conclusion: Summary of the findings and potential future directions for research.\n",
      "\n",
      "Impact:\n",
      "The Transformer model has had a profound impact on the field of NLP and beyond, becoming the foundation for many subsequent models, including BERT, GPT, and T5. This has led to significant improvements in various NLP tasks. Additionally, the self-attention mechanism has been successfully applied to other areas, such as computer vision.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [Computer Science Researcher] Task output: Title: Attention Is All You Need\n",
      "Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser\n",
      "Affiliations: Google Brain, Google Research, University of Toronto\n",
      "\n",
      "Key Contributions:\n",
      "1. Introduced the Transformer model, which is based entirely on self-attention mechanisms and does not require recurrent or convolutional layers.\n",
      "2. Introduced the self-attention mechanism that allows the model to weigh the importance of different words when encoding a word, enabling the capturing of long-range dependencies more effectively.\n",
      "3. Enabled more parallelization during training by removing recurrence, speeding up the process and improving efficiency on modern hardware.\n",
      "4. Achieved state-of-the-art results on various NLP tasks, demonstrating the effectiveness and versatility of the Transformer model.\n",
      "\n",
      "Structure of the Paper:\n",
      "1. Introduction: Discussion on the limitations of existing models and the motivation for the Transformer model.\n",
      "2. Model Architecture: Detailed description of the Transformer architecture, including the encoder-decoder structure, multi-head self-attention, and position-wise feed-forward networks.\n",
      "3. Self-Attention: Explanation of the self-attention mechanism and its computation.\n",
      "4. Positional Encoding: Discussion on the addition of positional encodings to input embeddings to capture the order of words.\n",
      "5. Training and Results: Description of the training process, datasets used, and the comparative results achieved by the Transformer model.\n",
      "6. Conclusion: Summary of the findings and potential future directions for research.\n",
      "\n",
      "Impact:\n",
      "The Transformer model has had a profound impact on the field of NLP and beyond, becoming the foundation for many subsequent models, including BERT, GPT, and T5. This has led to significant improvements in various NLP tasks. Additionally, the self-attention mechanism has been successfully applied to other areas, such as computer vision.\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Computer Science Writer\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Extract the most important topics and key components of the report\n",
      "                    developed by the Computer Science Researcher.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI now can give a great answer\n",
      "\n",
      "Final Answer:\n",
      "The research paper titled \"Attention Is All You Need\" by Ashish Vaswani and team from Google Brain, Google Research, and the University of Toronto introduces a revolutionary model known as the Transformer model. This model is entirely based on self-attention mechanisms, eliminating the need for recurrent or convolutional layers.\n",
      "\n",
      "The researchers introduced the self-attention mechanism, a novel method that allows the model to assign weights to different words when encoding a word, enabling it to capture long-range dependencies more effectively. This approach led to more parallelization during training by removing recurrence, which resulted in faster and more efficient training processes on modern hardware.\n",
      "\n",
      "The paper provides a detailed description of the Transformer architecture, which includes the encoder-decoder structure, multi-head self-attention, and position-wise feed-forward networks. It also explains the self-attention mechanism and the addition of positional encodings to input embeddings to capture the order of words. The training process, datasets used, and comparative results achieved by the Transformer model are also discussed.\n",
      "\n",
      "The impact of this work is profound in the field of Natural Language Processing (NLP), where the Transformer model has become the foundation for many subsequent models like BERT, GPT, and T5, leading to significant improvements in various NLP tasks. The self-attention mechanism, introduced in this paper, has also been successfully applied to other areas, such as computer vision.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [Computer Science Writer] Task output: The research paper titled \"Attention Is All You Need\" by Ashish Vaswani and team from Google Brain, Google Research, and the University of Toronto introduces a revolutionary model known as the Transformer model. This model is entirely based on self-attention mechanisms, eliminating the need for recurrent or convolutional layers.\n",
      "\n",
      "The researchers introduced the self-attention mechanism, a novel method that allows the model to assign weights to different words when encoding a word, enabling it to capture long-range dependencies more effectively. This approach led to more parallelization during training by removing recurrence, which resulted in faster and more efficient training processes on modern hardware.\n",
      "\n",
      "The paper provides a detailed description of the Transformer architecture, which includes the encoder-decoder structure, multi-head self-attention, and position-wise feed-forward networks. It also explains the self-attention mechanism and the addition of positional encodings to input embeddings to capture the order of words. The training process, datasets used, and comparative results achieved by the Transformer model are also discussed.\n",
      "\n",
      "The impact of this work is profound in the field of Natural Language Processing (NLP), where the Transformer model has become the foundation for many subsequent models like BERT, GPT, and T5, leading to significant improvements in various NLP tasks. The self-attention mechanism, introduced in this paper, has also been successfully applied to other areas, such as computer vision.\n",
      "\n",
      "\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "# Agents and Tools\n",
    "from crewai import Agent, Task, Crew\n",
    "\n",
    "### RESEARCHER ###\n",
    "\n",
    "# Agent\n",
    "reasarcher = Agent(\n",
    "    role=\"Computer Science Researcher\",\n",
    "    goal='Understanding the published research paper and extract the main insights',\n",
    "    backstory=\"\"\"You work at a Computer Science school.\n",
    "                  A new paper has been published and you need to extract\n",
    "                  the main information and provide the main insights to\n",
    "                  the students.You have a knack for dissecting complex\n",
    "                  research papers\"\"\",\n",
    "    verbose=True,\n",
    "    tools=[tool_instance],\n",
    "    allow_delegation=False\n",
    "    )\n",
    "\n",
    "\n",
    "# Tasks\n",
    "task_research = Task(\n",
    "    description=\"\"\"Conduct an in-depth analysis of the published research paper.\n",
    "                    Identify the key components and try to understand how each works.\"\"\",\n",
    "    expected_output=\"\"\"Your final answer must be a full analysis report of the research paper\"\"\",\n",
    "    agent=reasarcher,\n",
    "    output_file='researcher_analysis.md' \n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "### WRITER ###\n",
    "\n",
    "# Agent\n",
    "writer = Agent(\n",
    "    role=\"Computer Science Writer\",\n",
    "    goal= 'Craft easy to understand content for Computer Science students',\n",
    "    backstory=\"\"\"You are working on writing a summary of a research paper.\n",
    "                  Use the content of the Computer Science Researcher to develop\n",
    "                  a short comprehensive summary of the research paper\"\"\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    ")\n",
    "\n",
    "# Tasks\n",
    "task_writer = Task(\n",
    "    description=\"\"\"Extract the most important topics and key components of the report\n",
    "                    developed by the Computer Science Researcher.\"\"\",\n",
    "    expected_output=\"\"\"Your final answer must be a short comprehensive summary \n",
    "                        to understand report for Computer Science students\"\"\",\n",
    "    agent=writer,\n",
    "    output_file='writer_summary.md' \n",
    "\n",
    ")\n",
    "\n",
    "# CREWAI KICKOFF\n",
    "\n",
    "Crew = Crew(\n",
    "    agents=[reasarcher, writer],\n",
    "    tasks=[task_research, task_writer],\n",
    "    verbose=2)\n",
    "\n",
    "result = Crew.kickoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_kBHrE92Tj0l",
    "outputId": "0cf2f591-19bb-4ee0-e6b3-a14ee4a7d080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The research paper titled \"Attention Is All You Need\" by Ashish Vaswani and team from Google Brain, Google Research, and the University of Toronto introduces a revolutionary model known as the Transformer model. This model is entirely based on self-attention mechanisms, eliminating the need for recurrent or convolutional layers.\n",
      "\n",
      "The researchers introduced the self-attention mechanism, a novel method that allows the model to assign weights to different words when encoding a word, enabling it to capture long-range dependencies more effectively. This approach led to more parallelization during training by removing recurrence, which resulted in faster and more efficient training processes on modern hardware.\n",
      "\n",
      "The paper provides a detailed description of the Transformer architecture, which includes the encoder-decoder structure, multi-head self-attention, and position-wise feed-forward networks. It also explains the self-attention mechanism and the addition of positional encodings to input embeddings to capture the order of words. The training process, datasets used, and comparative results achieved by the Transformer model are also discussed.\n",
      "\n",
      "The impact of this work is profound in the field of Natural Language Processing (NLP), where the Transformer model has become the foundation for many subsequent models like BERT, GPT, and T5, leading to significant improvements in various NLP tasks. The self-attention mechanism, introduced in this paper, has also been successfully applied to other areas, such as computer vision.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "crewai",
   "language": "python",
   "name": "crewai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
